{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVQ9Q4lw4_XA"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.initializers import TruncatedNormal\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7xU1WDCtLvd"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prl-PotP4gNl"
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, hidden_size, num_heads):\n",
        "\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.projection_dim = hidden_size // num_heads\n",
        "        self.Q = layers.Dense(hidden_size)\n",
        "        self.K = layers.Dense(hidden_size)\n",
        "        self.V = layers.Dense(hidden_size)\n",
        "        self.out = layers.Dense(hidden_size)\n",
        "\n",
        "    def attention(self, query, key, value, mask):\n",
        "        \n",
        "        \n",
        "        d_k = tf.cast(query.shape[0], tf.float32)\n",
        "        weights = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "          weights += -1e9 * mask\n",
        "        \n",
        "        weights = keras.backend.softmax(weights)\n",
        "        #### completed this part ####\n",
        "        \n",
        "\n",
        "\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs, att_mask):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.separate_heads(self.Q(inputs)  , batch_size)  \n",
        "        key = self.separate_heads(self.K(inputs), batch_size)  \n",
        "        value = self.separate_heads(self.V(inputs) , batch_size) \n",
        "        attention, self.att_weights = self.attention(query, key, value, att_mask)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.hidden_size))\n",
        "        output = self.out(concat_attention)  \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCVLJBv951nW"
      },
      "source": [
        "#### Feed-Forward Sub-Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRnFXb0WwwSB"
      },
      "source": [
        "Unlike the original transformer, BERT uses \"GELU\" activation function. In this part you should implement the GELU activation function based on the paper provided to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzD7BjELQ--j"
      },
      "source": [
        "@tf.function\n",
        "\n",
        "def GELU(x):\n",
        "  #### completed this part ####\n",
        "  tanh_inp = np.sqrt(2/np.pi) * (x + 0.0044715 * np.pow(x, 3))\n",
        "  result = 0.5 * x * (1 + np.tanh(tanh_inp))\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqd6wedZXxzD"
      },
      "source": [
        "class FFN(layers.Layer):\n",
        "\n",
        "  def __init__(self, intermediate_size, hidden_size, drop_rate):\n",
        "\n",
        "    super(FFN, self).__init__()\n",
        "    self.intermediate = layers.Dense(intermediate_size, activation=GELU, kernel_initializer=TruncatedNormal(stddev=0.02))\n",
        "    self.out = layers.Dense(hidden_size, kernel_initializer=TruncatedNormal(stddev=0.02))\n",
        "    self.drop = layers.Dropout(drop_rate)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    #### completed this part ####\n",
        "    output = self.intermediate(inputs)\n",
        "    output = self.out(output)\n",
        "    output = self.drop(output)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDlwb3Ea6Aqc"
      },
      "source": [
        "#### Add & Norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4-UMLpDUkFa"
      },
      "source": [
        "In this part implement the add & norm blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TtnesNMOHUF"
      },
      "source": [
        "class AddNorm(layers.Layer):\n",
        "\n",
        "  def __init__(self, LNepsilon, drop_rate):\n",
        "    \n",
        "    super(AddNorm, self).__init__()\n",
        "    self.LN = layers.LayerNormalization(epsilon=LNepsilon)\n",
        "    self.dropout = layers.Dropout(drop_rate)\n",
        "\n",
        "  def call(self, sub_layer_in, sub_layer_out):\n",
        "    #### completed this part ####\n",
        "    output = np.add(sub_layer_in, sub_layer_out)\n",
        "    output = self.LN(output)\n",
        "    output = self.dropout(output)\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKqyg0J_WuTv"
      },
      "source": [
        "#### Residual connections\n",
        "\n",
        "Now put together all parts and build the encoder with the residual connections\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16zvGFBo_uaQ"
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "\n",
        "    def __init__(self, hidden_size, num_heads, intermediate_size, drop_rate=0.1, LNepsilon=1e-12):\n",
        "\n",
        "      super(Encoder, self).__init__()\n",
        "\n",
        "      #### completed this part ####\n",
        "      print(\"encoder ok...\")\n",
        "      self.multi_head_attention = MultiHeadAttention(hidden_size, num_heads)\n",
        "      print(\"encoder ok...\")\n",
        "      self.add_norm = AddNorm(LNepsilon, drop_rate)\n",
        "      print(\"encoder ok...\")\n",
        "      self.feed_forward = FFN(intermediate_size, hidden_size, drop_rate)\n",
        "      print(\"encoder ok...\")\n",
        "\n",
        "    def call(self, inputs, mask):\n",
        "\n",
        "      #### completed this part ####\n",
        "      output = self.multi_head_attention(inputs, mask)\n",
        "      output = self.add_norm(inputs, output)\n",
        "\n",
        "      output2 = self.feed_forward(output)\n",
        "      output2 = self.add_norm(output, output2)\n",
        "      return output2\n",
        "\n",
        "    def compute_mask(self, x, mask):\n",
        "\n",
        "      #### complete this part ####\n",
        "      \n",
        "\n",
        "      return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umQ878ho-6Hp"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTW-F4t9_x24"
      },
      "source": [
        "class BertEmbedding(layers.Layer):\n",
        "\n",
        "    def __init__(self, vocab_size, maxlen, hidden_size):\n",
        "\n",
        "      super(BertEmbedding, self).__init__()\n",
        "      self.TokEmb = layers.Embedding(input_dim=vocab_size, output_dim=hidden_size, mask_zero=True)\n",
        "      self.PosEmb = tf.Variable(tf.random.truncated_normal(shape=(maxlen, hidden_size), stddev=0.02))\n",
        "      self.LN = layers.LayerNormalization(epsilon=1e-12)\n",
        "      self.dropout = layers.Dropout(0.1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "      #### completed this part ####\n",
        "      output = self.TokEmb(inputs)\n",
        "      output = output + self.PosEmb\n",
        "      output = self.dropout(self.LN(output))\n",
        "      return output\n",
        "\n",
        "    def compute_mask(self, x, mask=None):\n",
        "      m = 1-tf.cast(self.TokEmb.compute_mask(x), tf.float32)\n",
        "      m = m[:, tf.newaxis, tf.newaxis, :]\n",
        "      return m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPjWqcH-ytQP"
      },
      "source": [
        "The \"pooler\" is the last layer you need to put in place.\n",
        "For each input sentence, the pooler changes the hidden states of the last encoder layer (which have the shape [batch size, sequence lenght, hidden size]) into a vector representation (which has the shape [batch size, hidden size]).\n",
        "The pooler does this by giving a dense layer the hidden state that goes with the first token, which is a special token at the beginning of each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O719umhMz_UH"
      },
      "source": [
        "class Pooler(layers.Layer):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "\n",
        "      super(Pooler, self).__init__()\n",
        "      self.dense = layers.Dense(hidden_size, activation='tanh')\n",
        "\n",
        "    def call(self, encoder_out):\n",
        "\n",
        "      #### complete this part ####\n",
        "      output = self.dense(encoder_out)\n",
        "      return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P8zt_tFojZY"
      },
      "source": [
        "Now you should complete the **create_BERT** function in the cell below. This function gets BERT's hyper-parameters as its inputs and return a BERT model. \n",
        "Note that the returned model must have two outputs (just like the pre-trained BERTs): \n",
        "- The hidden states of the last encoder layer\n",
        "- Output of the pooler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9tD7UtNfZ4p"
      },
      "source": [
        "def create_BERT(vocab_size, maxlen, hidden_size, num_layers, num_att_heads, intermediate_size, drop_rate=0.1):\n",
        "\n",
        "  \"\"\"\n",
        "  creates a BERT model based on the arguments provided\n",
        "\n",
        "        Arguments:\n",
        "        vocab_size: number of words in the vocabulary\n",
        "        maxlen: maximum length of each sentence\n",
        "        hidden_size: dimension of the hidden state of each encoder layer\n",
        "        num_layers: number of encoder layers\n",
        "        num_att_heads: number of attention heads in the multi-headed attention layer\n",
        "        intermediate_size: dimension of the intermediate layer in the feed-forward sublayer of the encoders\n",
        "        drop_rate: dropout rate of all the dropout layers used in the model\n",
        "        returns: \n",
        "  \"\"\"\n",
        "  \n",
        "  #### completed this part ####\n",
        "\n",
        "  inputs = tf.keras.Input(shape=(None, maxlen))\n",
        "  print(\"ok...\")\n",
        "  emb = BertEmbedding(vocab_size, maxlen, hidden_size)(inputs)\n",
        "\n",
        "  print(\"ok...\")\n",
        "  encoder_out = Encoder(hidden_size, num_att_heads,\n",
        "                        intermediate_size, drop_rate)(emb)\n",
        "  print(\"ok...\")\n",
        "  for i in range(num_layers-1):\n",
        "    encoder_out = Encoder(hidden_size, num_att_heads,\n",
        "                          intermediate_size, drop_rate)(encoder_out)\n",
        "\n",
        "  print(\"ok...\")\n",
        "  pooler_out = Pooler(hidden_size)(emb)\n",
        "\n",
        "  print(\"ok...\")\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=(emb, pooler_out)) \n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKBEBTI6sFKu"
      },
      "source": [
        "We will use the Rotten tomatoes critic reviews dataset for this assignment. The zip file is provided to you. Unzip it and run the cells below to split the dataset in training and test sets and prepare it for feeding to the bert model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUn-48AVXbfR"
      },
      "source": [
        "train_reviews, test_reviews = pd.read_csv('train_reviews.csv').values[:, 1:], pd.read_csv('test_reviews.csv').values[:, 1:]\n",
        "(train_texts, train_labels), (test_texts, test_labels)  = (train_reviews[:,0],train_reviews[:,1]), (test_reviews[:,0],test_reviews[:,1]) \n",
        "train_texts = [s.lower() for s in train_texts]\n",
        "test_texts = [s.lower() for s in test_texts] \n",
        "aprx_vocab_size = 20000\n",
        "cls_token = '[cls]'\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_generator=train_texts,\n",
        "                                                        target_vocab_size=aprx_vocab_size,\n",
        "                                                        reserved_tokens=[cls_token])                                               "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhTvBa9ntO7b"
      },
      "source": [
        "In the following cell, you need to complete the implementation of the encode_sentence function. This function takes as input a sentence and an integer representing the maximum length of the sentence and returns a list of token ids. To implement this function, follow these steps:\n",
        "\n",
        "-Use the trained tokenizer to encode the input sentence and obtain a list of token ids.\n",
        "\n",
        "-Pad the token id list with zeros to the maximum length specified.\n",
        "\n",
        "-Prepend the id of the special token to the beginning of the token id list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzO4yiJSmIRs"
      },
      "source": [
        "def encode_sentence(s, maxlen):\n",
        "\n",
        "  #### completed this part ####\n",
        "\n",
        "  tokens = tokenizer.encode(s)\n",
        "  \n",
        "  if len(tokens) > maxlen:\n",
        "    tok_id_list = [1] + tokens[:maxlen]\n",
        "  else:\n",
        "    tok_id_list = [1] + tokens + [0] * (maxlen - len(tokens))\n",
        "\n",
        "\n",
        "  return tok_id_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL-PTRJPYnPb"
      },
      "source": [
        "MAXLEN = 32\n",
        "x_train = np.array([encode_sentence(x, MAXLEN) for x in train_texts], dtype=np.int64)\n",
        "x_test = np.array([encode_sentence(x, MAXLEN) for x in test_texts], dtype=np.int64)\n",
        "y_train = train_labels.astype(np.int64)\n",
        "y_test = test_labels.astype(np.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBirx1Fbvv-k"
      },
      "source": [
        "Now use the functional api and the **create_BERT** function you implemented earlier to create a classifier for the movie reviews dataset.\n",
        "Note that the intermediate layer in the feed-forward sub-layer of the encoders is set to $4\\times H$ in the original BERT implementation, where $H$ is the hidden layer size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZOW4L9gBqvc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95183c51-ced8-4ef6-934e-52c4dc207edd"
      },
      "source": [
        "hidden_size = 768\n",
        "num_heads = 12\n",
        "num_layers = 12\n",
        "vocab_size = tokenizer.vocab_size  \n",
        "intermediate_size = 4 * hidden_size\n",
        "\n",
        "#### complete this part ####\n",
        "model = create_BERT(vocab_size, MAXLEN, hidden_size, num_layers,\n",
        "                    num_heads, intermediate_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok...\n",
            "ok...\n",
            "ok...\n",
            "ok...\n",
            "ok...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwBQt1bFBwYh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "df0378f5-2b24-461a-901e-521c5c5e7e69"
      },
      "source": [
        "model.compile(tf.keras.optimizers.Adam(learning_rate=5e-5), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-c69ab47b9ee2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IwB37mHByJ0"
      },
      "source": [
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=128,\n",
        "    epochs=2,\n",
        "    validation_data=(x_test, y_test)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPfMQS1ZsHHk"
      },
      "source": [
        "### Attention Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwGoqnHXadiF",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1c9c43a-706f-4dc1-9aa5-4c50eac27958"
      },
      "source": [
        "#@title Run this!\n",
        "import sys\n",
        "\n",
        "!test -d bertviz_repo && echo \"FYI: bertviz_repo directory already exists, to pull latest version uncomment this line: !rm -r bertviz_repo\"\n",
        "# !rm -r bertviz_repo # Uncomment if you need a clean pull from repo\n",
        "!test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo\n",
        "if not 'bertviz_repo' in sys.path:\n",
        "  sys.path += ['bertviz_repo']\n",
        "\n",
        "from bertviz import head_view\n",
        "\n",
        "def call_html():\n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
        "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bertviz_repo'...\n",
            "remote: Enumerating objects: 1625, done.\u001b[K\n",
            "remote: Counting objects: 100% (321/321), done.\u001b[K\n",
            "remote: Compressing objects: 100% (113/113), done.\u001b[K\n",
            "remote: Total 1625 (delta 226), reused 220 (delta 208), pack-reused 1304\u001b[K\n",
            "Receiving objects: 100% (1625/1625), 198.36 MiB | 17.16 MiB/s, done.\n",
            "Resolving deltas: 100% (1068/1068), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p5EleW-6EaN"
      },
      "source": [
        "In order to use bertviz, we need to obtain the attention weights in the encoders of the BERT model implemented in the previous section. To do this, you need to complete the implementation of the get_att_weights function in the following cell. This function takes as input a model (the trained BERT-based model from the previous section) and a list of tokens (an encoded sentence). Here's what you need to do:\n",
        "\n",
        "-Feed the input token list to the model to generate the attention weights for that input.\n",
        "\n",
        "-Access the att_weights attribute of the MultiHeadAttention sub-layer of each encoder in the model and add them all to a list.\n",
        "\n",
        "-Return the list (which should be a list of Tensors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUsR5r4Z-Pd7"
      },
      "source": [
        "def get_att_weights(model, tok_id_list):\n",
        "  \n",
        "#### complete this part ####\n",
        "\n",
        "  return att_weights\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaV7YOtuBQeC"
      },
      "source": [
        "import torch\n",
        "def get_att_tok(model, sent):\n",
        "\n",
        "  maxlen = model.layers[0].input_shape[0][-1]\n",
        "  encoded_toks = encode_sentence(sent, maxlen)\n",
        "  att_weights = get_att_weights(model, encoded_toks)\n",
        "  pad_start_idx = np.min(np.where(np.array(encoded_toks) == 0))\n",
        "  toks = encoded_toks[:pad_start_idx]\n",
        "  atts = []\n",
        "  for att in att_weights:\n",
        "    layer_att = torch.FloatTensor(att[:, :, :pad_start_idx, :pad_start_idx].numpy())\n",
        "    atts.append(layer_att)\n",
        "  toks = [tokenizer.decode([m]) for m in toks]\n",
        "  return toks, atts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attention visualization\n",
        "now give a sample sentence in the context of giving your opinion about a movie and visualize the attention. for example \"I liked that movie\""
      ],
      "metadata": {
        "id": "MOhd420dk8Ld"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65xPcS1VIWyc"
      },
      "source": [
        "sentence = \"Your sentence\"\n",
        "toks, atts = get_att_tok(model, sentence.lower())\n",
        "call_html()\n",
        "head_view(atts, toks)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}